{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the 31 audit related variables (ARV)\n",
    "ARV=[         \n",
    "       'Industry Specialization (sale)_national',\n",
    "       'Industry Specialization (sale)_city', \n",
    "    \n",
    "    'Office Size_sales', \n",
    "    'Big4',\n",
    "    \n",
    "       'New Client', 'Tenure', \n",
    "    \n",
    "    'Same MSA', 'INTEGRATED AUDIT',\n",
    "       'Historical is accelerated filer',\n",
    "       'Busy', 'WorkloadCompression',\n",
    "       'HI_pre_sq', 'Auditor resigned_revised', \n",
    "    \n",
    "    'Log_AuditFee', 'Log_TaxFee',\n",
    "       'Log_AuditRelatedFee', 'Log_OtherFee', 'NonAuditFeeRatio', 'Influence',\n",
    "       'AbnormalLAF', 'Log Audit Report Lag', 'Due_to_Auditor',\n",
    "       'Going concern', 'SOX404auditorWeak',\n",
    "    \n",
    "       'Disc. Accruals', 'Abs (Disc. Accruals)', 'Abs(Accruals)',\n",
    "       'Abs(Accruals/CFO)', 'DD Residual', \n",
    "    'Small Profit', 'Prior ROA meet']\n",
    "\n",
    "\n",
    "# list of ARV that are continous variables \n",
    "Cont_arv=[      'Industry Specialization (sale)_national',\n",
    "       'Industry Specialization (sale)_city', \n",
    "    \n",
    "    'Office Size_sales', \n",
    "   'Tenure', \n",
    "    \n",
    "  'WorkloadCompression',\n",
    "       'HI_pre_sq', \n",
    "    \n",
    "    'Log_AuditFee', 'Log_TaxFee',\n",
    "       'Log_AuditRelatedFee', 'Log_OtherFee', 'NonAuditFeeRatio', 'Influence',\n",
    "       'AbnormalLAF', 'Log Audit Report Lag', \n",
    "    \n",
    "       'Disc. Accruals', 'Abs (Disc. Accruals)', 'Abs(Accruals)',\n",
    "       'Abs(Accruals/CFO)', 'DD Residual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors=ARV\n",
    "\n",
    "# standardize continuous variables\n",
    "def Transform(X):\n",
    "    for name in list(Cont_arv):\n",
    "        X[name]=(X[name]-X[name].mean())/X[name].std()\n",
    "        X=X[['CIK Number_x','Data Year - Fiscal_x','Label','RES']+ARV]     \n",
    "    return X\n",
    "\n",
    "\n",
    "Test_Year=[]\n",
    "Cost=[]\n",
    "Round=[]\n",
    "BestModel=[]\n",
    "AUC=[]\n",
    "\n",
    "testyears=list(range(2015, 2018))\n",
    "costratios=[1,20,30,40,50]\n",
    "\n",
    "for TestYear in testyears:\n",
    "    MasterPath='/Users/username/'#user name is masked for double-blind review\n",
    "    FilePath=\"FolderPath/Analysis/Correct consecutive restatement/Train and Test/Set_\"+str(TestYear)+\".xlsx\" #FolderPath is masked for double-blind review\n",
    "    Path=MasterPath+FilePath\n",
    "    df=pd.read_excel(Path)\n",
    "    trans_df=Transform(df)\n",
    "    \n",
    "    #train and test for final results\n",
    "    test=trans_df[trans_df['Data Year - Fiscal_x']==TestYear]\n",
    "    train=trans_df[trans_df['Data Year - Fiscal_x']<=TestYear-2]\n",
    "\n",
    "    #split training data into training for validation and testing for validation\n",
    "    vali_test=trans_df[trans_df['Data Year - Fiscal_x']==TestYear-2]\n",
    "    vali_train=trans_df[trans_df['Data Year - Fiscal_x']<=TestYear-4]\n",
    "\n",
    "    test_X=test[predictors]\n",
    "    test_y=test['RES']\n",
    "    train_X=train[predictors]\n",
    "    train_y=train['RES']\n",
    "\n",
    "    vali_test_X=vali_test[predictors]\n",
    "    vali_test_y=vali_test['RES']\n",
    "    vali_train_X=vali_train[predictors]\n",
    "    vali_train_y=vali_train['RES']\n",
    "\n",
    "    \n",
    "    param_grid = {'base_estimator': [DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=5)], \n",
    "       'n_estimators': [30, 50, 70],\n",
    "      'learning_rate': [0.5, 1, 1.5]} # This is a grid of hyperparameters for AdaBoost\n",
    "\n",
    "\n",
    "    Base_Estimator=[]\n",
    "    N_Estimators=[]\n",
    "    Learning_Rate=[]\n",
    "    Algorithm=[]\n",
    "\n",
    "\n",
    "    Tune_AUC=[]\n",
    " \n",
    "\n",
    "    vali_train_RES_indices = vali_train[vali_train.RES==1].index  #get the index of RES instances in vali_train \n",
    "    vali_train_NORES_indices = vali_train[vali_train.RES==0].index #get the index of NON-RES instances in vali_train \n",
    "    \n",
    "    for num_split in costratios:\n",
    "\n",
    "        \n",
    "        splitted_NORES=np.array_split(np.random.permutation(vali_train_NORES_indices),num_split) #randomly split the NON-RES instances in vali-train into num_split batches following Perols et al. (2017)\n",
    "\n",
    "        #Begin Hyperparameter Tuning under each cost ratio \n",
    "\n",
    "        for estimator in param_grid['base_estimator']:\n",
    "            for number in param_grid['n_estimators']:\n",
    "                for rate in param_grid['learning_rate']:\n",
    "                    tune=AdaBoostClassifier(base_estimator=estimator, n_estimators=number, learning_rate=rate) #for model with each and every combination of hyperparameters\n",
    "                    y_score_array=[] # to store probability predictions from models trained by each subsample\n",
    "                    for splitted_NORES_indices in splitted_NORES: # here we adopt the OU method from Perols et al. (2017)\n",
    "\n",
    "                        under_sample_indices = np.concatenate([vali_train_RES_indices,splitted_NORES_indices]) # for each batch, create an undersampled training set as indicated in Perols et al. (2007)\n",
    "                        \n",
    "                        vali_train_under_sample_y = vali_train_y.loc[under_sample_indices]\n",
    "                        vali_train_under_sample_X = vali_train_X.loc[under_sample_indices]\n",
    "\n",
    "                        probas_=tune.fit(vali_train_under_sample_X, vali_train_under_sample_y).predict_proba(vali_test_X)\n",
    "                        y_score=probas_[:, 1]\n",
    "                        y_score_array.append(y_score)\n",
    "\n",
    "                    #average the prediction probabilities from different batches        \n",
    "                    y_average_score=[]\n",
    "                    for i in range(len(vali_test_y)):\n",
    "                        average_score=np.mean(y_score_array, axis=0)[i]\n",
    "                        y_average_score.append(average_score)\n",
    "\n",
    "\n",
    "                    auc=roc_auc_score(vali_test_y, y_average_score)\n",
    "                    \n",
    "                    Tune_AUC.append(auc)\n",
    "                    Base_Estimator.append(estimator)\n",
    "                    N_Estimators.append(number)\n",
    "                    Learning_Rate.append(rate)\n",
    "\n",
    "\n",
    "        HyperTune_Results=pd.DataFrame()\n",
    "        HyperTune_Results['Base_Estimator']=Base_Estimator\n",
    "        HyperTune_Results['N_Estimators']=N_Estimators\n",
    "        HyperTune_Results['Learning_Rate']=Learning_Rate\n",
    "        HyperTune_Results['AUC']=Tune_AUC\n",
    "\n",
    "        #The tuned model is the one with the combination that maximizes AUC in the hold-out validation set\n",
    "        tunedresults=HyperTune_Results[HyperTune_Results['AUC']==HyperTune_Results['AUC'].max()].reset_index()\n",
    "\n",
    "        bestmodel=AdaBoostClassifier(base_estimator=tunedresults['Base_Estimator'][0],\n",
    "                                    n_estimators=tunedresults['N_Estimators'][0],\n",
    "                                    learning_rate=tunedresults['Learning_Rate'][0]) #use the hyperparameters selected above\n",
    "\n",
    "\n",
    "        print(\"for test year \"+str(TestYear)+\" the best model is \"+ str(bestmodel))\n",
    "\n",
    "        #End hyperparameter tuning under each cost ratio\n",
    "\n",
    "        #Use the best model to make classification - trained on the training set and test on the hold-out test set\n",
    "\n",
    "        train_RES_indices = train[train.RES==1].index  \n",
    "        train_NORES_indices =train[train.RES==0].index  \n",
    "        \n",
    "        seed=3\n",
    "        \n",
    "        for eachround in range(seed): \n",
    "\n",
    "            splitted_NORES=np.array_split(np.random.permutation(train_NORES_indices),num_split)\n",
    "\n",
    "            y_score_array=[] # used to collect prediction probability from each split \n",
    "            y_average_score=[] # will store average prediction probability \n",
    "            y_pred_all=[] # will store the prediction results using the average prediciton probability based on threshold of 0.5\n",
    "\n",
    "\n",
    "            for splitted_NORES_indices in splitted_NORES: # here we adopt the OU method from Perols et al. (2017)\n",
    "                under_sample_indices = np.concatenate([train_RES_indices,splitted_NORES_indices]) # for each batch, create an undersampled training set\n",
    "                train_under_sample_y = train_y.loc[under_sample_indices]\n",
    "                train_under_sample_X = train_X.loc[under_sample_indices]\n",
    "\n",
    "                probas_ = bestmodel.fit(train_under_sample_X, train_under_sample_y).predict_proba(test_X) \n",
    "                #obtain the probability prediction \n",
    "                y_score=probas_[:, 1]\n",
    "                y_score_array.append(y_score)\n",
    "\n",
    "\n",
    "            num_test=len(test_y) \n",
    "            for i in range(num_test):\n",
    "                average_score=np.mean(y_score_array, axis=0)[i] # take the average of the probability scores from all splits\n",
    "                y_average_score.append(average_score)\n",
    "\n",
    "\n",
    "            auc=roc_auc_score(test_y, y_average_score)\n",
    "            \n",
    "            AUC.append(auc)\n",
    "            Test_Year.append(TestYear)\n",
    "            BestModel.append(bestmodel)\n",
    "            Round.append(eachround)\n",
    "            Cost.append(num_split)\n",
    "\n",
    "results=pd.DataFrame()\n",
    "results['Test Year']=Test_Year\n",
    "results['AUC']=AUC\n",
    "results['Misclassification Cost Ratio']=Cost\n",
    "results['Round']=Round\n",
    "results['Tuned Model']=BestModel\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
